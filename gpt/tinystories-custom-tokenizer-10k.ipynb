{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14073044,"sourceType":"datasetVersion","datasetId":8958219}],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import BPE\nfrom tokenizers.trainers import BpeTrainer\nfrom tokenizers.pre_tokenizers import Whitespace","metadata":{"id":"qrewtNWm78NH","trusted":true,"execution":{"iopub.status.busy":"2025-12-10T04:13:53.439397Z","iopub.execute_input":"2025-12-10T04:13:53.439998Z","iopub.status.idle":"2025-12-10T04:13:56.860996Z","shell.execute_reply.started":"2025-12-10T04:13:53.439976Z","shell.execute_reply":"2025-12-10T04:13:56.860379Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# --------- User configurable settings ---------\nDATA_TXT = \"/kaggle/input/tinystories/tinystories_train.txt\"\nTOKENIZER_FILE = \"/kaggle/input/tinystories/tinystories_train.json\"\nMAX_CHARS = 10_000_000 # read at most 10 million characters\nVOCAB_SIZE = 4096\nBATCH_SIZE = 64\nBLOCK_SIZE = 256\nMAX_ITERS = 5000\nEVAL_INTERVAL = 200\nLEARNING_RATE = 3e-4\nEVAL_ITERS = 50 # reduced for speed during eval\nGRAD_CLIP = 1.0\n\n\n# Model size reduced to better match small dataset\nN_EMBD = 256\nN_HEAD = 4\nN_LAYER = 4\nDROPOUT = 0.1","metadata":{"id":"m5jZn3pX8W3g","outputId":"21ec8d0c-4542-4c35-f044-6e4eeb0e7a7b","trusted":true,"execution":{"iopub.status.busy":"2025-12-10T04:13:56.862028Z","iopub.execute_input":"2025-12-10T04:13:56.862405Z","iopub.status.idle":"2025-12-10T04:13:56.867044Z","shell.execute_reply.started":"2025-12-10T04:13:56.862376Z","shell.execute_reply":"2025-12-10T04:13:56.866268Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Device (Kaggle friendly)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T04:13:56.867849Z","iopub.execute_input":"2025-12-10T04:13:56.868034Z","iopub.status.idle":"2025-12-10T04:13:56.935012Z","shell.execute_reply.started":"2025-12-10T04:13:56.868019Z","shell.execute_reply":"2025-12-10T04:13:56.934281Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"TOKENIZER_FILE = \"/kaggle/working/tokenizer.json\"\n\n# ---------------- Tokenizer (train once) ----------------\nif not os.path.exists(TOKENIZER_FILE):\n    print(\"Training tokenizer...\")\n    tokenizer = Tokenizer(BPE(unk_token='[UNK]'))\n    tokenizer.pre_tokenizer = Whitespace()\n    trainer = BpeTrainer(\n        vocab_size=VOCAB_SIZE,\n        special_tokens=['[UNK]', '[CLS]', '[SEP]', '[PAD]', '[MASK]']\n    )\n    tokenizer.train([DATA_TXT], trainer)\n\n    tokenizer.save(TOKENIZER_FILE)\n    print(\"Tokenizer saved to\", TOKENIZER_FILE)\n\nelse:\n    print(\"Loading existing tokenizer...\")\n    tokenizer = Tokenizer.from_file(TOKENIZER_FILE)\n\nvocab_size = tokenizer.get_vocab_size()\nprint(\"Vocab size:\", vocab_size)\n\nencode = lambda s: tokenizer.encode(s).ids\ndecode = lambda ids: tokenizer.decode(ids)\n","metadata":{"id":"MOkz8Kug8g_q","trusted":true,"execution":{"iopub.status.busy":"2025-12-10T04:13:56.936700Z","iopub.execute_input":"2025-12-10T04:13:56.937169Z","iopub.status.idle":"2025-12-10T04:13:57.979717Z","shell.execute_reply.started":"2025-12-10T04:13:56.937150Z","shell.execute_reply":"2025-12-10T04:13:57.979031Z"}},"outputs":[{"name":"stdout","text":"Training tokenizer...\n\n\n\nTokenizer saved to /kaggle/working/tokenizer.json\nVocab size: 4096\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ------------ Load data ------------\nwith open(DATA_TXT, 'r', encoding='utf-8') as f:\n    text = f.read(MAX_CHARS)\n\n\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9 * len(data))\ntrain_data = data[:n]\nval_data = data[n:]","metadata":{"id":"wz4WNd_J8ipX","trusted":true,"execution":{"iopub.status.busy":"2025-12-10T04:13:57.980417Z","iopub.execute_input":"2025-12-10T04:13:57.980619Z","iopub.status.idle":"2025-12-10T04:14:03.637609Z","shell.execute_reply.started":"2025-12-10T04:13:57.980604Z","shell.execute_reply":"2025-12-10T04:14:03.636999Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# --------------- helpers ---------------\ndef get_batch(split):\n    data_src = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data_src) - BLOCK_SIZE, (BATCH_SIZE,))\n    x = torch.stack([data_src[i:i+BLOCK_SIZE] for i in ix])\n    y = torch.stack([data_src[i+1:i+BLOCK_SIZE+1] for i in ix])\n    return x.to(device), y.to(device)","metadata":{"id":"wQHG1dpu8lLW","trusted":true,"execution":{"iopub.status.busy":"2025-12-10T04:14:03.638315Z","iopub.execute_input":"2025-12-10T04:14:03.638514Z","iopub.status.idle":"2025-12-10T04:14:03.643627Z","shell.execute_reply.started":"2025-12-10T04:14:03.638497Z","shell.execute_reply":"2025-12-10T04:14:03.642881Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"@torch.no_grad()\ndef estimate_loss(model):\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(EVAL_ITERS)\n        for k in range(EVAL_ITERS):\n            X, Y = get_batch(split)\n            _, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean().item()\n    model.train()\n    return out","metadata":{"id":"lP0kiKRu8nQ9","trusted":true,"execution":{"iopub.status.busy":"2025-12-10T04:14:03.644366Z","iopub.execute_input":"2025-12-10T04:14:03.644603Z","iopub.status.idle":"2025-12-10T04:14:03.661407Z","shell.execute_reply.started":"2025-12-10T04:14:03.644585Z","shell.execute_reply":"2025-12-10T04:14:03.660471Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# --------------- Rotary embeddings ---------------\nclass RotaryPositionalEmbeddings(nn.Module):\n    def __init__(self, head_dim, max_seq_len=BLOCK_SIZE):\n        super().__init__()\n        # standard RoPE inverse frequency\n        inv_freq = 10000 ** (-torch.arange(0, head_dim, 2).float() / head_dim)\n        positions = torch.arange(max_seq_len).float().unsqueeze(1)\n        angles = positions * inv_freq.unsqueeze(0) # (max_seq_len, head_dim/2)\n        # store cos and sin as buffers so they move with module.to(device)\n        self.register_buffer('cos_cached', angles.cos())\n        self.register_buffer('sin_cached', angles.sin())\n    \n    \n    def forward(self, seq_len):\n        # return slices shaped (seq_len, head_dim//2)\n        return self.cos_cached[:seq_len, :], self.sin_cached[:seq_len, :]","metadata":{"id":"KHY2Hj-y8sey","outputId":"7bb59f49-1d57-4216-b3f4-21f1cce5f08d","trusted":true,"execution":{"iopub.status.busy":"2025-12-10T04:14:03.662546Z","iopub.execute_input":"2025-12-10T04:14:03.662775Z","iopub.status.idle":"2025-12-10T04:14:03.672594Z","shell.execute_reply.started":"2025-12-10T04:14:03.662751Z","shell.execute_reply":"2025-12-10T04:14:03.671881Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def apply_rotary_emb(x, cos, sin):\n    # x: (B, T, head_dim)\n    d = x.shape[-1] // 2\n    x1 = x[..., :d]\n    x2 = x[..., d:]\n    # cos and sin are (T, d) need to broadcast to (B, T, d)\n    # make shapes explicit for readability\n    cos = cos.unsqueeze(0)\n    sin = sin.unsqueeze(0)\n    out1 = x1 * cos - x2 * sin\n    out2 = x1 * sin + x2 * cos\n    return torch.cat([out1, out2], dim=-1)","metadata":{"id":"ZF7694hp8vIl","trusted":true,"execution":{"iopub.status.busy":"2025-12-10T04:14:03.673359Z","iopub.execute_input":"2025-12-10T04:14:03.673596Z","iopub.status.idle":"2025-12-10T04:14:03.684649Z","shell.execute_reply.started":"2025-12-10T04:14:03.673575Z","shell.execute_reply":"2025-12-10T04:14:03.684049Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# --------------- Attention and Transformer blocks ---------------\nclass Head(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.head_size = head_size\n        self.key = nn.Linear(N_EMBD, head_size, bias=False)\n        self.query = nn.Linear(N_EMBD, head_size, bias=False)\n        self.value = nn.Linear(N_EMBD, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)))\n        self.dropout = nn.Dropout(DROPOUT)\n        self.rotary = RotaryPositionalEmbeddings(head_size, max_seq_len=BLOCK_SIZE)\n    def forward(self, x):\n        B, T, C = x.shape\n        k = self.key(x)\n        q = self.query(x)\n        # get cos and sin for sequence length T\n        cos, sin = self.rotary(T)\n        q = apply_rotary_emb(q, cos, sin)\n        k = apply_rotary_emb(k, cos, sin)\n        # scaled dot product\n        wei = q @ k.transpose(-2, -1) * (self.head_size ** -0.5)\n        mask = self.tril[:T, :T].to(wei.device)\n        wei = wei.masked_fill(mask == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n        wei = self.dropout(wei)\n        v = self.value(x)\n        out = wei @ v\n        return out\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, N_EMBD)\n        self.dropout = nn.Dropout(DROPOUT)\n    \n    \n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n        nn.Linear(n_embd, 4 * n_embd),\n        nn.GELU(),\n        nn.Linear(4 * n_embd, n_embd),\n        nn.Dropout(DROPOUT),\n        )\n    \n    \n    def forward(self, x):\n        return self.net(x)\n\n\nclass Block(nn.Module):\n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedForward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n    \n    \n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x","metadata":{"id":"HgZqyQhf8xdz","trusted":true,"execution":{"iopub.status.busy":"2025-12-10T04:14:03.686564Z","iopub.execute_input":"2025-12-10T04:14:03.686873Z","iopub.status.idle":"2025-12-10T04:14:03.699803Z","shell.execute_reply.started":"2025-12-10T04:14:03.686852Z","shell.execute_reply":"2025-12-10T04:14:03.699146Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# --------------- GPT Model ---------------\nclass GPTLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, N_EMBD)\n        self.blocks = nn.Sequential(*[Block(N_EMBD, n_head=N_HEAD) for _ in range(N_LAYER)])\n        self.ln_f = nn.LayerNorm(N_EMBD)\n        self.lm_head = nn.Linear(N_EMBD, vocab_size)\n        self.apply(self._init_weights)\n    \n    \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n    \n    \n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        tok_emb = self.token_embedding_table(idx)\n        x = tok_emb\n        x = self.blocks(x)\n        x = self.ln_f(x)\n        logits = self.lm_head(x)\n        if targets is None:\n            return logits, None\n        B, T, C = logits.shape\n        logits = logits.view(B * T, C)\n        targets = targets.view(B * T)\n        loss = F.cross_entropy(logits, targets)\n        return logits, loss\n    \n    \n    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -BLOCK_SIZE:]\n            logits, _ = self(idx_cond)\n            logits = logits[:, -1, :] / temperature\n            if top_k is not None:\n                v, _ = torch.topk(logits, top_k)\n                logits[logits < v[:, [-1]]] = -float('Inf')\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, idx_next), dim=1)\n        return idx","metadata":{"id":"KEHv3DXi8z1G","trusted":true,"execution":{"iopub.status.busy":"2025-12-10T04:14:03.700508Z","iopub.execute_input":"2025-12-10T04:14:03.700786Z","iopub.status.idle":"2025-12-10T04:14:03.715932Z","shell.execute_reply.started":"2025-12-10T04:14:03.700772Z","shell.execute_reply":"2025-12-10T04:14:03.715193Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# --------------- instantiate model, optimizer, scaler ---------------\nmodel = GPTLanguageModel()\nmodel = model.to(device)\nprint(\"Model parameters: \", sum(p.numel() for p in model.parameters()) / 1e6, \"M\")\n\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\nimport torch.amp\n\nscaler = torch.amp.GradScaler(enabled=(device == 'cuda'))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T04:14:03.716611Z","iopub.execute_input":"2025-12-10T04:14:03.716843Z","iopub.status.idle":"2025-12-10T04:14:08.301017Z","shell.execute_reply.started":"2025-12-10T04:14:03.716827Z","shell.execute_reply":"2025-12-10T04:14:08.300430Z"}},"outputs":[{"name":"stdout","text":"Model parameters:  5.257728 M\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"for it in range(MAX_ITERS):\n    if it % EVAL_INTERVAL == 0 or it == MAX_ITERS - 1:\n        losses = estimate_loss(model)\n        print(f\"step {it}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n    \n    xb, yb = get_batch('train')\n    \n    optimizer.zero_grad(set_to_none=True)\n    \n    # mixed precision when on CUDA\n    with torch.amp.autocast(device_type='cuda', enabled=(device == 'cuda')):\n        _, loss = model(xb, yb)\n    \n    if device == 'cuda':\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n        scaler.step(optimizer)\n        scaler.update()\n    else:\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n        optimizer.step()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T04:14:08.301777Z","iopub.execute_input":"2025-12-10T04:14:08.302053Z","iopub.status.idle":"2025-12-10T04:28:23.629963Z","shell.execute_reply.started":"2025-12-10T04:14:08.302037Z","shell.execute_reply":"2025-12-10T04:28:23.629130Z"}},"outputs":[{"name":"stdout","text":"step 0: train loss 8.3880, val loss 8.3906\nstep 200: train loss 3.6833, val loss 3.6101\nstep 400: train loss 3.1909, val loss 3.1489\nstep 600: train loss 2.9209, val loss 2.9222\nstep 800: train loss 2.7335, val loss 2.7757\nstep 1000: train loss 2.5818, val loss 2.6745\nstep 1200: train loss 2.4638, val loss 2.5977\nstep 1400: train loss 2.3653, val loss 2.5408\nstep 1600: train loss 2.2860, val loss 2.4894\nstep 1800: train loss 2.2133, val loss 2.4455\nstep 2000: train loss 2.1477, val loss 2.4309\nstep 2200: train loss 2.0728, val loss 2.3897\nstep 2400: train loss 2.0270, val loss 2.3974\nstep 2600: train loss 1.9713, val loss 2.3778\nstep 2800: train loss 1.9161, val loss 2.3616\nstep 3000: train loss 1.8713, val loss 2.3589\nstep 3200: train loss 1.8313, val loss 2.3710\nstep 3400: train loss 1.8006, val loss 2.3610\nstep 3600: train loss 1.7532, val loss 2.3785\nstep 3800: train loss 1.7192, val loss 2.3641\nstep 4000: train loss 1.6843, val loss 2.3736\nstep 4200: train loss 1.6603, val loss 2.3678\nstep 4400: train loss 1.6206, val loss 2.3878\nstep 4600: train loss 1.5784, val loss 2.3891\nstep 4800: train loss 1.5507, val loss 2.3956\nstep 4999: train loss 1.5297, val loss 2.4043\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# --------------- generation example ---------------\nprint(\"Generating story...\")\nstart_prompt = \"Once upon a time\"\ncontext = torch.tensor([encode(start_prompt)], dtype=torch.long, device=device)\nwith torch.no_grad():\n    generated = model.generate(context, max_new_tokens=300, temperature=1.0, top_k=50)[0].tolist()\nprint(decode(generated))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T04:28:23.630874Z","iopub.execute_input":"2025-12-10T04:28:23.631134Z","iopub.status.idle":"2025-12-10T04:28:26.910151Z","shell.execute_reply.started":"2025-12-10T04:28:23.631107Z","shell.execute_reply":"2025-12-10T04:28:26.909520Z"}},"outputs":[{"name":"stdout","text":"Generating story...\nOnce upon a time there was a lucky bridge in the sand . It was the sun and it was safe to fly away . One day , when the other fish found a path was dark outside . It was hungry and wanted to eat and find food . The thin stone decided to sit in the water and look for something yummy . It was a little old dog . The dog wanted to eat some food . So the two walked around the bridge but n one of the rocks they both got tired . The two were scared , but then the big bear found a way home . The squirrel said , \" Wow , that ' s amazing !\" They all laughed and continued playing together . When the mist of the squirrel returned , the bird thanked everyone for looking at him . They decided it was time to be home and safe . The squirrel said goodbye to the squirrel , and ran off to try his new friend . Once there was a boy called Jack . He was going on an adventure . Every day Jack packed up his special treasures and headed out . Today , Jack couldn ' t wait to take his special suit . He called for a walk outside . Jack went outside his hut and sho wn his friends . He was outside going on long trip . Jack spent the whole summer days flying around and exploring . Every night he would go to bed , looking forward to the day . Once there was a little girl called Milly . She had a lovely garden , just like she was growing in the forest . She was very tired from all the she could find .\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# --------------- optional: save model checkpoint ---------------\nCKPT_PATH = \"tinystories_gpt.pt\"\ntry:\n    torch.save({'model_state_dict': model.state_dict(), 'tokenizer': TOKENIZER_FILE}, CKPT_PATH)\n    print(\"Saved checkpoint to\", CKPT_PATH)\nexcept Exception as e:\n    print(\"Could not save checkpoint:\", e)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T04:28:26.910869Z","iopub.execute_input":"2025-12-10T04:28:26.911127Z","iopub.status.idle":"2025-12-10T04:28:26.962801Z","shell.execute_reply.started":"2025-12-10T04:28:26.911109Z","shell.execute_reply":"2025-12-10T04:28:26.962058Z"}},"outputs":[{"name":"stdout","text":"Saved checkpoint to tinystories_gpt.pt\n","output_type":"stream"}],"execution_count":15}]}